
---

"### üè• Teste T√©cnico - (Intuitive Care)"
"**Candidato:**" Joseph Borges Morais
"**Perfil:**" Acad√™mico de Bacharelado em Sistemas de Informa√ß√£o - 6¬∫ Semestre (IFBA)
"**Foco:**" Back-end Development & Data Engineering

---

"### üèóÔ∏è Arquitetura do Projeto"
O projeto foi estruturado seguindo o modelo de pipeline `"**ETL (Extract, Transform, Load)**"`, garantindo a separa√ß√£o de responsabilidades entre a coleta, o tratamento e a persist√™ncia dos dados.

---

"### üöÄ Guia de Execu√ß√£o (Passo a Passo)"

Para garantir a integridade dos dados, siga a ordem rigorosa abaixo:

"**1. Prepara√ß√£o do Ambiente:**"

```bash
"**# Criar e ativar o ambiente virtual**"
python -m venv .venv
source .venv/Scripts/activate 

"**# Instalar depend√™ncias core**"
pip install pandas requests beautifulsoup4 sqlalchemy psycopg2-binary

```

"**2. Configura√ß√£o do Banco de Dados:**"

* Crie um banco de dados no PostgreSQL chamado `intuitive_db`.
* Execute o script `"**schema.sql**"` para estruturar as tabelas, chaves prim√°rias e √≠ndices.

"**3. Execu√ß√£o do Pipeline ETL:**"

* `"**Extra√ß√£o:**"` `python main.py` (Download e extra√ß√£o via Web Scraping resiliente).
* `"**Tratamento:**"` `python transformacao.py` (Limpeza e padroniza√ß√£o inicial).
* `"**Enriquecimento:**"` `python enriquecimento.py` (Cruzamento de dados entre operadoras e despesas).
* `"**Intelig√™ncia:**"` `python agregacao.py` (Gera√ß√£o do arquivo de KPIs estat√≠sticos).

"**4. Carga de Dados (Data Loading):**"

* Execute `python importacao.py` para realizar a ingest√£o automatizada dos arquivos CSV para o PostgreSQL. Este script trata inconsist√™ncias de tipos e valores nulos em tempo real.

"**5. Valida√ß√£o e An√°lise:**"

* Execute as queries contidas em `"**analise.sql**"` no seu cliente SQL (pgAdmin) para extrair os relat√≥rios de crescimento e Market Share.

---

"### üß† Decis√µes de Engenharia e Trade-offs"

"#### 1. Processamento e Mem√≥ria (Etapa 1.2)"

* "**Estrat√©gia:**" Processamento Incremental por arquivos.
* "**Justificativa:**" Para suportar o volume massivo da ANS (centenas de milhares de linhas por trimestre), evitamos o carregamento em lote na RAM, prevenindo erros de `"**Stack Overflow**"` ou `"**Out of Memory**"`.

"#### 2. Modelagem de Dados (Etapa 3.2)"

* "**Abordagem:**" `"**Op√ß√£o B (Tabelas Normalizadas)**"`.
* "**Justificativa:**"
* `"**Escalabilidade:**"` Reduz a redund√¢ncia de dados cadastrais (Raz√£o Social, UF) que se repetem milh√µes de vezes nas despesas.
* `"**Integridade:**"` Uso de `"**Foreign Keys (FK)**"` para garantir que nenhuma despesa seja √≥rf√£ de uma operadora cadastrada.



"#### 3. Precis√£o Financeira (Etapa 3.2)"

* "**Tipo de Dado:**" `"**DECIMAL(18,2)**"`.
* "**Justificativa:**" Em sistemas de back-end cont√°bil, o uso de `FLOAT` √© evitado devido √† imprecis√£o bin√°ria em grandes somas. O `DECIMAL` garante que c√°lculos de bilh√µes de reais sejam exatos.

---

"### üîç Qualidade e Higiene de Dados (Etapa 1.3 & 3.3)"

Para garantir a confiabilidade dos relat√≥rios, implementei:

* `"**Regex Sanitization:**"` Extra√ß√£o de metadados diretamente dos nomes dos arquivos para evitar erros de digita√ß√£o nas planilhas.
* `"**Normaliza√ß√£o de Tipos:**"` Convers√£o autom√°tica de strings/v√≠rgulas em formatos num√©ricos compat√≠veis com o PostgreSQL durante a importa√ß√£o.
* `"**Deduplica√ß√£o Inteligente:**"` L√≥gica de `"**keep last**"` para manter apenas a vers√£o mais atualizada da raz√£o social de cada operadora.

---

"### üìä SQL Analytics (F√≥rmulas de Neg√≥cio)"
O sistema utiliza a seguinte l√≥gica para o c√°lculo de crescimento percentual entre per√≠odos:

---

"### üìÇ Estrutura de Arquivos"

* `"**saida/**"`: Artefatos gerados (CSVs e ZIPs consolidados).
* `"**schema.sql**"`: Defini√ß√£o de tabelas, PKs, FKs e √çndices de performance.
* `"**importacao.py**"`: Script de carga com tratamento de exce√ß√µes e normaliza√ß√£o de headers.

---


