### ğŸ¥ Teste TÃ©cnico â€“ Intuitive Care

**Candidato:** Joseph Borges Morais  
**Perfil:** AcadÃªmico de Bacharelado em Sistemas de InformaÃ§Ã£o â€“ 6Âº Semestre (IFBA)  
**Foco:** Back-end Development & Data Engineering

---

### ğŸ—ï¸ Arquitetura do Projeto

O projeto foi estruturado seguindo o modelo de pipeline **ETL (Extract, Transform, Load)**, garantindo a separaÃ§Ã£o de responsabilidades entre a coleta, o tratamento e a persistÃªncia dos dados.

---

### ğŸš€ Guia de ExecuÃ§Ã£o (Passo a Passo)

Para garantir a integridade dos dados, siga rigorosamente a ordem abaixo:

#### 1. PreparaÃ§Ã£o do Ambiente

```bash
# Criar o ambiente virtual
python -m venv .venv

# Ativar o ambiente virtual
# Windows
.venv\Scripts\activate

# Linux/macOS
source .venv/bin/activate

# Instalar dependÃªncias principais
pip install pandas requests beautifulsoup4 sqlalchemy psycopg2-binary
```

---

#### 2. ConfiguraÃ§Ã£o do Banco de Dados

- Crie um banco de dados PostgreSQL chamado `intuitive_db` (dentro do arquivo de importacao.py, altere o campo com a senha do seu Postgres).
- Execute o script `schema.sql` para criar tabelas, chaves primÃ¡rias, chaves estrangeiras e Ã­ndices.

---

#### 3. ExecuÃ§Ã£o do Pipeline ETL

- **ExtraÃ§Ã£o:** `python extracao.py` (Download e extraÃ§Ã£o via Web Scraping resiliente).
- **Tratamento:** `python transformacao.py` (Limpeza e padronizaÃ§Ã£o inicial).
- **Enriquecimento:** `python enriquecimento.py` (Cruzamento de dados entre operadoras e despesas).
- **InteligÃªncia:** `python agregacao.py` (GeraÃ§Ã£o do arquivo de KPIs estatÃ­sticos).

---

#### 4. Carga de Dados (Load)

Execute:
```bash
python importacao.py
```

---

#### 5. ValidaÃ§Ã£o e AnÃ¡lise

Execute as queries contidas em `analise.sql` no cliente SQL (ex: pgAdmin).

---

### ğŸ§  DecisÃµes de Engenharia e Trade-offs

#### Processamento e MemÃ³ria
EstratÃ©gia: Processamento Incremental por arquivos.

Justificativa: Para suportar o volume massivo da ANS (centenas de milhares de linhas por trimestre), evitamos o carregamento em lote na RAM, prevenindo erros de **Stack Overflow** ou **Out of Memory**.

#### Modelagem de Dados
Abordagem: **OpÃ§Ã£o B (Tabelas Normalizadas)**.

Justificativa:

**Escalabilidade:** Reduz a redundÃ¢ncia de dados cadastrais (RazÃ£o Social, UF) que se repetem milhÃµes de vezes nas despesas.

**Integridade:** Uso de **Foreign Keys (FK)** para garantir que nenhuma despesa seja Ã³rfÃ£ de uma operadora cadastrada.

#### PrecisÃ£o Financeira
Tipo de Dado: **DECIMAL(18,2)**.

Justificativa: Em sistemas de back-end contÃ¡bil, o uso de FLOAT Ã© evitado devido Ã  imprecisÃ£o binÃ¡ria em grandes somas. O DECIMAL garante que cÃ¡lculos de bilhÃµes de reais sejam exatos.

### ğŸ” Qualidade e Higiene de Dados (Etapa 1.3 & 3.3)

Para garantir a confiabilidade dos relatÃ³rios, implementei:

**Regex Sanitization:** ExtraÃ§Ã£o de metadados diretamente dos nomes dos arquivos para evitar erros de digitaÃ§Ã£o nas planilhas.

**NormalizaÃ§Ã£o de Tipos:** ConversÃ£o automÃ¡tica de strings/vÃ­rgulas em formatos numÃ©ricos compatÃ­veis com o PostgreSQL durante a importaÃ§Ã£o.

**DeduplicaÃ§Ã£o Inteligente:** LÃ³gica de **keep last** para manter apenas a versÃ£o mais atualizada da razÃ£o social de cada operadora.

---

### ğŸ“Š SQL Analytics

```sql
((valor_periodo_atual - valor_periodo_anterior) 
 / valor_periodo_anterior) * 100
```

---

### ğŸ“‚ Estrutura de Arquivos

- `saida/`
- `schema.sql`
- `importacao.py`

